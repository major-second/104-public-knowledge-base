- 前置
  - [[markov-chebshev]]
    - 尤其是[[markov-chebshev#chernoff bounds]]
  - [[bernoulli-binom]]
  - [[entropy#KL Divergence]]
- 参考
  - [[central-limit]]
    - 但是那里是分布，而这里关注一个安全的“界”，超过多少误差的概率
  - [[hoeffding#main contents]]
    - 但是那里经过[[hoeffding#lemma]]放缩，所以比 [这个](#concentration) 松
# concentration
- 考虑[[bernoulli-binom]]
- 非负，则满足[[markov-chebshev#chernoff bounds]]说的条件
- 则套公式$P_0:=P(\sum x_i \ge n(p+\epsilon)) \le inf_t \{e^{-nt(p+\epsilon)}Ee^{t\sum x_i}\}$
- 代入得$\le inf_t\{ e^{-nt(p+\epsilon)}(pe^t+q)^n\}$
- 现在考察$-t(p+\epsilon) + ln(pe^t+q)$极值（求导）
  - $-p-\epsilon +\frac{pe^t}{pe^t+q}=0$
  - $(p+\epsilon)(pe^t+q)=pe^t$
  - $(p+\epsilon)q=pe^t(q-\epsilon)$
  - $t=ln(\frac{p+\epsilon}{1-p-\epsilon})-ln(\frac{p}{1-p})$
  - $e^t=(p+\epsilon)q/p(q-\epsilon)$
  - $pe^t+q=q/(q-\epsilon)$
- $P_0\le(e^{-(p+\epsilon)ln(\frac{p+\epsilon}{q-\epsilon})+(p+\epsilon)ln(\frac p{q})+ln(\frac q{q-\epsilon})})^n=e^{n(-(p+\epsilon)ln(p+\epsilon)+(q-\epsilon)lnq+(p+\epsilon)lnp-(q-\epsilon)ln(q-\epsilon))}=e^{-n((p+\epsilon)ln(p+\epsilon)+(q-\epsilon)ln(q-\epsilon)-(p+\epsilon)lnp-(q-\epsilon)lnq)}:=e^{-nD_B^{(e)}(p+\epsilon ||p )}$
- 其中$D_B^{(e)}$表示两个参数分别为$p+\epsilon,p$的伯努利分布的[[entropy#KL Divergence]]
- 这个bound可以称为concentration
- 注意这是单侧界
# 和独立性的关系
- [[1-prob/independent]]
- 直接去掉肯定不行。比如所有变量都一样显然没有concentration了
- 但是可以调整：有放回和无放回，参考[[with-replacement]]
  - 直觉上：无放回concentration更强，更有所谓的“抵消”效果