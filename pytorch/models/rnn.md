- 前置[[models/basics]]
- 举例：[官网GRU文档](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)
- [结构，有哪些weights矩阵等](https://zhuanlan.zhihu.com/p/30844905)
- 和常见的模块比较不同的地方
  - 如果不`batch_first=True`，则是默认序列长$L$对应维度在前
  - 即使`batch_first=True`了，也和[[conv]]不同：不是`N, Channel`，而是`N, Length, Channel`
- 对于神经网络训练，一般需要较多数据，可牺牲[[1-1-prob/independent]]，[参考](https://stats.stackexchange.com/questions/490813/is-it-better-to-split-sequences-into-overlapping-or-non-overlapping-training-sam)
- 如果在forward时手动给hidden置0，要特别注意`to(<input>.device)`
  - 时至2022.12，[[trainer]]并不能自动处理这个。这是[[leaky-abstraction]]
- 注意一般不用[[batchnorm]]，参考[[rl-batch-norm]]原理，每一步分布不同
  - [参考](https://zhuanlan.zhihu.com/p/398255616)，需要魔改或[[layernorm]]