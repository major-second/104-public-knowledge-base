- 前置
  - [[category]]
  - [[eda]]
  - [[visualization]]
  - [[preprocessing]]
# integrity
- 不能有[[information-leak]]
- 不能疯狂[[overfit]]
  - 如非必要，勿增实体（各种乱七八糟操作）
  - 疯狂堆叠表达式
    - 各种运算各种操作（复杂度高）
    - 比如“看起来这个分布图上呈现某某特征所以我乘一下这个，除一下那个”，就不好
    - 每一个操作、公式都要有实际含义、依据
    - 当你提出feature不work，主要要反省根本思想、建模上哪里出了问题，而不是马上开始torture表达式
  - 疯狂增加待定参数，使得feature在validation上好，那说白了就是过拟合validation set而已
    - 如[[4-design]]中，“1-12月给12个参数”，可能能过拟合几年的（验证集）股市数据，但没有任何经济学意义
  - 参考[[2-eval]], [[naming]]
  - 可能被称为（贬义）“数据挖掘”
# 意义和优美性
- 首先要有一个建模
  - 可以是
    - mental model，跑不起来的
      - 对事情背后的逻辑有一个自己大概的认识（有点像思维中有“隐变量”）
    - 一定程度上能跑起来的，如[[PIN]]基于的
  - 甚至可以形成一个脑海中的贝叶斯网络（参考[[12-uncertainty]]），有因果联系等
  - 然后对着这个模型试图用公式表达feature，每个公式、记号的目的都是（最简洁地）表达你想要的意思
- 物理量的“量纲”非常重要
  - [[normalization]], [[preprocessing]]
  - 不同量纲不能相加
  - [[dimensionless]]
# 常见手段
- 基础手段
  - 数字特征（[[cov]], [[variance]], [[expectation]]等）
    - 如：滚动[[rolling]]求均值、方差等
    - MA, EMA, MA之差等
      - 这些是线性的，其实有点想到[[deep-learning-basics]]的feature extraction
      - horizon当然是可调参数
  - [[time-series]]
    - 速度、加速度、位移、路程等
    - [[MA]], [[autoregressive]]等传统模型
- 有时跟[[preprocessing]]不分家
  - [[data-science/normalization]]
  - [[data-science/residual]]
  - [[12-robust]]
    - 缩尾winsorization
  - 特判[[general-principles/special-case]]
  - outliers
- [[arithmetic-calculation]]
- [[polynomial-features]]
- [[binning]]
# 举例
- [海通高频因子](https://www.htsec.com/jfimg/colimg/upload/20181106/32441541468174586.pdf)
- [alpha101](https://arxiv.org/abs/1601.00991)
  - [列表](https://github.com/wukan1986/expr_codegen/blob/main/examples/alpha101.txt)
    - 参考[[meta-programming]]
  - [分析](https://www.zhihu.com/column/c_1317426644055502848)