- [S4](https://arxiv.org/pdf/2111.00396.pdf)
  - [S4 code](https://github.com/state-spaces/s4)
- 前置
  - [[ssm]]
    - 主要是降低计算复杂度，实现本质上类似的模型
# 预备
- HiPPO
   - [[ssm]]中的 $A$指定一个值
   - 避免 [[first-order-linear]] -> [[gradient-issue]]
     - 随机训 $A$，几乎必定出问题。显然随机的话$A$几乎必然可对角化且特征值很多远离1的，那就不好了
       - 由此一个简单想法：可否限制特征值模不能太远离1？
       - 一种提议：对角阵，特征值模全为1，那就相当于input一维被拆成若干数（分量）之和，每个分量独立转圈（正余弦），显然不够合理
       - 如果特征值模可能不为1，那就加上一些爆炸/消失
       - 总之完全[[diagonalization]]，肯定不行
- [[discrete-continuous]]
  - $\bar A = (I-\Delta A/2)^{-1}(I+\Delta A/2)$
  - 理解方式
    - 注意$A$和$t$无关是常量矩阵
      - 提前提一嘴：后面说的[[symmetry#平移]]
    - $\Delta\to 0$，显然
      - $\bar A\approx A$
      - 也可联系上$e^{A}$的定义和[[first-order-linear]]
    - 相比$I+\Delta A$的好处？“取中点减小误差”，类似数值积分的梯形法等
      - [[general-principles/special-case]]理解
        - $(0.99^{-1}*1.01)^{100}$相比$1.01^{200}$显然更接近 $e^2$
        - $e^2=7.38905609893$
        - $(\frac {101}{100})^{200}=7.31601785183$
        - $(\frac {101}{99})^{100}=7.38954874865$
- [[conv]]
  - 显然利用[[symmetry#平移]]（时间轴上）
  - 卷积核 $\bar K=(\bar C\bar B,\cdots,\bar C\bar A^{L-1}\bar B)$
- [[matrix-power]]
  - 简单[[diagonalization]]会导致 HiPPO矩阵 数值性质不好
  - 如果有[[real-symmetric#spectral-theorem]]就好了（$V$为[[unitary-matrix]]）
# 核心算法
- 三个要点
- 该文不是严格 HiPPO，而是以其为基础进行[[parametric-inference]]参数化
  - 参考[[hypothesis-space]]
- [[complexity]]分析作为表，相比只会比点数的，确实质量高
  - 加粗表示理论最优
    - [[parallelism]]自然是 `Yes`最优，233
# architecture-details
- $(\Lambda-PQ^*,B,C)$，由于[[diagonalization]]，复杂度变为$5N=\Theta (N)$了而不是$\Theta(N^2)$
- 高维？[[orthogonal-decomposition]]思想
  - 要么处理feature维（线性层，$H^2$）
  - 要么处理sequence length维（[[s4#核心算法]]层，$HN$）
- 总体 $(B,L,H)$ seq2seq类似[[rnn]]等竞争对手
- 核心部分线性
  - 总体可以类比一种[[conv]]
# 关于A的实验
- 是否train：train则 [[hypothesis-space]]更大，training set 可很高，但泛化未必好
- 在什么范围train（是否NPLR）
- 初始化：HiPPO or not