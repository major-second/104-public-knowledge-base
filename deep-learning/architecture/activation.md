- 不加激活函数的问题：[[linearity]]复合还是[[linearity]]，没有额外表达能力
- [参考](https://zhuanlan.zhihu.com/p/30510596)
- [参考](https://zhuanlan.zhihu.com/p/172254089)
- 看什么标准
  - 导数数值
    - 避免神经元死亡，[[gradient-issue]]等
      - leaky relu
      - [[soft#softplus]]
      - ELU
  - 输出值域
    - 如sigmoid用于[[gate]]
    - 如ELU, leaky relu防止[[data#恒正问题]]
- 拓展
  - 防[[overfit]]加噪声
    - noisy relu
    - Randomized LeakyReLU
    - 类比[[dropout]], [[augment#操作方法]]加噪声等手段
  - 可学习：PReLU
    - 反而增大参数量，小心[[overfit]]
  - [[data#恒正问题]]