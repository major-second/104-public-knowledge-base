- [一篇知乎技巧trick总结](https://zhuanlan.zhihu.com/p/95081141)
- 常见超参
  - [[lr-scheduler]]
  - [[loss-function]]
  - [[batchsize]]
  - [[weight-init]]
## 动力学
- 参见[[2-eval]]
- 实践中常常：训练，验证，测试
  - 看`val, test`的loss曲线，性能曲线，可判断是
    - 欠拟合（如学习率、EPOCH不够等）
    - 不稳定不收敛（如学习率太大，BS太小）
    - 还是过拟合
  - 这些是最常见的情况
    - 所以训练过程中最重要的参数就是**学习率、BS、EPOCH**等超参
    - 三者相互关联并非独立变化
    - 比如：BS太大可能导致同样EPOCH更新次数变少、BS和LR需要联系着变化等
  - 曲线趋势不典型（比如没有上升段）可能的原因：输出间隔太短
    - 实践中可能需要[[trainer]]的一些设置，例如`limit_train_batches`
- 欠拟合、学习率太大BS太小都好解决（反正训练集摆在那，你拟合好就行）
  - 你连过拟合训练集都做不到那一定代码除了严重问题，别想别的了
  - 可以先[[general-principles/debug]]过拟合特别小数据集
- 过拟合的解决不平凡，从而引出“正则化”
- 训练集一般需要`shuffle`
  - 尤其一种情况：训练集中batch数量很多，故每个epoch只随机取出一部分，然后epoch结束即做val，避免val间隔太大看不到曲线趋势。此时当然必须`shuffle`
### 防过拟合
- [正则化总结](https://zhuanlan.zhihu.com/p/69025058)
- 原理参考[[2-eval]], [[overfit]]等
- early stop（即：看验证集，别等验证集loss回升了才停）
  - 其实践参考[[tensorboard]], [[checkpoint]]等
- 增加正则化惩罚项到[[loss-function]]
  - 参考[[11-feature-selection]]中LASSO，岭回归
    - 其实[[11-feature-selection]]本身也是很有用的防过拟合手段！
  - l1: 可以导致稀疏性
  - l2
    - 类似岭回归。是比较常见、一般的罚项
    - 由于$x^2$导数正比于$x$，所以对于最简单的情况，weight decay和l2正则化损失是对应的
      - 当然，对于复杂一些的优化器如Adam就不一定了
      - 参考[[deep-learning/optimization]]
      - torch实践：在[[basics/optimization]]中实现，如`SGD(其它参数, weight_decay=1e-2)`
#### dropout
- 参数：常常`0.1, 0.5`等值
## 结构
- 以2的倍数为间隔调MLP隐层宽度
  - [[enumerate]]前当然要确定全集的界
- [[residual]]使得可以增加更多层
  - 哈哈，你把residual看作一个trick？也行吧哈哈
## [[feature-engineering]]
- [[feature-engineering]]提到的clip，有时对线性和对MLP都能提升，但是对MLP提升多
  - 推测可能是MLP需要数据的某种“密度”大，input极差大会容易boil它
- input整体的normalization
  - 当然要使用**训练集**的mean, std，参考[[information-leak]]
  - 注意小心极差/标准差极大的维度，做了反而不行（所以要和刚刚的winsorize结合）
- 中间层[[batchnorm]]等，参考[[batchnorm]]