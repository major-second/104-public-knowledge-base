- 对抗，多方目标不同
  - 棋牌当然比曲棍球简单（物理规则确定，状态和动作少）
  - 机器人足球倒是不少人做（robocup）
## 6.1 Game Theory
- 三种stance
  - 经济学：太多人了，站在个人角度，基本上是“被动”的
  - 把对方当环境（当成“雨天”），没法考虑到对方在积极打你
  - 显式model出adversary（本章）
- 最常见的：
  - deterministic, two-player, turn-taking（回合制）
    - 玩家max先动，min后动
    - 每一个人自己的行动称为ply（因为move有时有歧义，指的是两个人各一步）
  - perfect information（棋是牌不是）
  - zero-sum games
    - 和恒定，比如你赢它输或它赢你输，和都是1
    - 恒定为1和恒定为0无本质区别
- 严格定义：初态，某个状态下谁动、能怎么动、动的结果（transition）。终态集合以及每个终态对应每个人得分
  - 和[[3-search]]搜索问题有联系也有区别
  - 区别
    - 比如这里某个状态可能你动也可能它动，而不是都由你决定下一步怎么做
    - 比如最后得分针对每个人各一个
  - 联系
    - state space graph概念仍存在，也有节点、边
    - 也有search tree. 称为game tree，也可能无限
- 由于和为常数，两个人，所以可以用一个数字概括结果，越大则玩家max越好，min越差
## 6.2 Optimal Decisions in Games
- 只有赢输两种结果：[[4-local-search]]的AND-OR search，即相当于认为环境有不确定性，你需要在任何情况下都赢
- minimax是AND-OR的简单拓展（游戏结果可能有很多种时）
  - 就是逆向归纳（递归），从后往前，假设两人都足够聪明
  - 递归出口：$MINIMAX(s)=UTILITY(s,max),if\quad IS-TERMINAL(s)$
  - 递归过程：根据这一步是谁行动，取子节点的min或max
    - 这一步是max行动，就取子节点的max
    - 这一步是min行动，就取子节点的min
  - 最优性：非常保险，别人如果没那么聪明，你只会结果更好
    - 但是不一定能“充分利用别人的不聪明攫取最大收益”
  - 伪码：min和max互相递归调用
  - 递归展开时相当于深搜，开销太大
  - 可以非常方便地扩展到多人博弈和非零和（递归调用时反正都“站在别人角度想”即可）
- 联盟
  - 多人，自发联盟：C强，A和B都打C，没有任何协议但是自己形成
  - 双人非零和也有可能
  - 签订显式契约等：破坏时可能有点损失让人要权衡一下
### alpha-beta
- 直观例子体会思想：[[464-can-i-win]]中的`break`
  - 核心思想：短路求值
  - 有些点不用看也不影响结果
- 参考[[enumerate#pruning]]
- 典型解说
  - 你下了这一步，再下一步对面可能能将死你
  - 你就不用考察对面其它下法。反正对面会将死你
- 名称来源：互相调用的递归函数`MIN-VALUE, MAX-VALUE`多了两个参数$\alpha,\beta$
  - $\alpha$：之前祖先max的最好动作造成结果
  - $\beta$：之前祖先min的最好动作造成结果
- $\alpha$只是考察一部分可能性时max的最好结果，所以肯定小于全局的最大。所以，你如果发现某个min节点的子节点（即此时min可选动作）存在一个比$\alpha$还低的，那肯定就要剪掉该节点
  - 即$\alpha$意为min的子节点（动作）至少应怎样（否则触发剪）
- $\beta$只是考察一部分可能性时min的最好结果，所以肯定大于全局的最小。所以，你如果发现某个max节点的子节点（即此时max可选动作）存在一个比$\beta$还高的，那肯定就要剪掉该节点
  - 即$\beta$意为max的子节点（动作）至多应怎样（否则触发剪）
- min的动作导致太小说明max出臭棋，max的动作导致太大说明min出臭棋
- 实际算法实现
  - 枚举max的子节点时
    - 每次发现比原来好（结果大）的节点时，就知道自己有更好的选择，同时可能更新$\alpha$，提升“判臭棋”标准
      - $\alpha$变大，即该点子孙处更容易判max下了臭棋导致min能取很小的点。具体什么叫很小？小于$\alpha$
      - 也有可能提升不了。因为$\alpha\leftarrow MAX(\alpha,v)$，如果不比祖先大，就提升不了
      - 之后子孙的任何min节点，一旦出现了（min的）子节点比$\alpha$小，就“向上汇报这是max的臭棋”
    - 发现任何一个值大于等于$\beta$的动作就直接往上return，告诉min这是步臭棋
### 拓展
- 考察什么才能多剪
  - 为了更多剪枝，应该一来就把“判臭棋”变得很容易，也就是先考察“自认为好”的节点
    - 比如max先考察自认为会比较大的节点，就可以把$\alpha$堆得大，之后min的ply能小于$\alpha$就向上汇报剪
  - 效率：深度变为暴力搜的$1/2$或$3/4$等，大约能成比例减少
    - 一些简单的排序就可以效率很高，非常接近理论极限
    - 更强的：动态变
      - 根据过去动作选择顺序
      - 或先“试探”然后“重新决定顺序”（iterative deepening）
      - killer move heuristic
- “手割”手法减少开销：缓存状态，忽略过程不同，能大概翻倍能考察的深度
- 象棋围棋还是不行，状态太多
  - 香农：Type A strategy：考察很多可能，但不仔细考察，用一些方法估值
  - Type B：考察很深，多剪剪枝
  - 象棋往往type A，围棋B（动作太多）
## 6.3 Heuristic Alpha–Beta Tree Search
- `IS-TERMINAL`改成人为定义的`IS-CUTOFF`（和深度$d$有关）
  - 所以递归时要传深度参数
- `UTILITY`相应改成估值函数`EVAL`，有点像[[3-search]]的heuristic估值
  - 要求：terminal时相等（类似于[[3-search]]中到了终点输出0），其它时候在输和赢中间
  - 比如可以使用feature（剩几个兵？）
  - 可以事先计算“两兵对一兵胜率”这种
  - 也可以使用线性组合等，或者更复杂的模型
  - 比如入门教材中给棋子、棋形、安全的王估值
  - 估值函数保序比较重要，具体数值次要
  - 估值函数可以人给也可以机器学（机器几个小时学人几个世纪）
- Cutting off search：通过`IS-CUTOFF`早些停下深搜
  - 最直接的：搜索指定深度$d$以内的
  - 提升鲁棒性：逐次提升$d$，预防超时没输出（而且优化考察顺序）
  - 改进：只有quiescent（下一步不会大变）才估值
    - 举例：比如认为只有吃大子才算大变
  - horizon effect：考察时间有限（短视），超过了看不见，就可能“拖字诀”
    - 改进：如果有这种必吃对方子的，就强行提升深度，“你拖也没用”
- forward pruning：type B，暴力多剪剪
  - 比如beam search（但非常危险，很容易剪掉好的）
  - PROBCUT：通过子节点值估计这个节点要剪的概率，差不多得了就剪（而不是$\alpha-\beta$一样确定要剪再剪）
  - late move reduction：对顺序靠后的moves不太报期望，搜浅一点（除非“女人，你引起了我的注意”）
- 举例：国际象棋：暴力搜 - $\alpha-\beta$ - 手割表 - GPU - 估值 - 结束游戏的动作表等，从5步到30多步
- 定式、查表也很有用（对于开局和终局）
- 对于象棋残局
  - 可以搜
  - 还能retrograde，从终局往前倒推
  - 这里目的往往是推出一个**有特征的子集**的所有输赢情况（比如王象马对王），之后方便查表
## 6.4 Monte Carlo Tree Search
- 围棋难估值，所以需要MCTS
- 用模拟结果期望作为估值函数
- simulation/payout/rollout同义
- 最naive版本
  - 没有中途的估值，必须下出输赢结果
  - 且是随机一人一步模拟
- 随机地一人一步没那么好：需要playout policy
  - 这个playout可能使用NN或一些启发（如逃打吃）
- **什么时候开始**playout？
  - 最简单的思想：pure MC search
  - 第一步不经过playout而是任选
  - 后面使用playout
  - 看哪个第一步（对应后面使用playout引出的结果）最好
- selection policy：并不是后面全用playout，也不是第一步纯随机
  - explore states that have had few playouts
  - exploit已有获得更精确估值
  - 两者需要平衡
- 先有search tree，然后每个iteration四步
  - selection：在已有树上（无论根据explore原则还是exploit原则）走几步到叶子
  - expansion: 叶子下新增一至多个新叶子
  - simulation: 用新生成的叶子为起点模拟（但不放入树，模拟完就扔）
  - back-propagation: 更新所有祖先的总rollout数字段、胜利rollout数字段
- selection policy例子
  - $UCB1(n)=U(n)/N(n)+C\sqrt{logN(PARENT(n))/N(n)}$
  - 第一项对应利用，第二项对应探索
  - log使得到后来越来越倾向于利用
  - $C$可调。纯理论最好是$\sqrt 2$
  - AlphaZero还加入move probability项
- 最终返回UCB1高的，而不是单纯“胜率高”（否则不确定性大）
- 经验：围棋这种空间大的/eval不好写的不适合$\alpha-\beta$适合mcts
  - 很多随机playout也更稳定，不会被一个误区坑死
- mcts也可以用eval（playout到一定步数不模拟了）
- 可以从零开始自走凭空获取知识，并用NN进行distill
- 属于type B，很深，所以可能会忽略一些唯一关键手
- `The general idea of simulating moves into the future, observing the outcome, and using the outcome to determine which moves are good ones is one kind of reinforcement learning`