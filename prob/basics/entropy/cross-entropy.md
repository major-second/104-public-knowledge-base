- [参考](https://zhuanlan.zhihu.com/p/149186719)
- 用于多分类
  - gt $1,0,0$，预测完全一样是$1,0,0$，一致，交叉熵为0
  - gt $1,1,1$，预测完全一样时[[kl-divergence]]为0
  - 所以优化交叉熵可以让分布接近
- 参考
  - [[kl-divergence]]