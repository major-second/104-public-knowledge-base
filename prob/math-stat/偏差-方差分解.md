- $E(\bar X-\mu)^2=E((\bar X-E\bar X)+(E\bar X-\mu))^2=Var\bar X+0(交叉项)+Bias^2(注意E\bar X-\mu是常数)$
- 实质
  - 方差(Var)和均方误差(Mean Squared Error, MSE)，实际上就是“针对的那个”（相对谁作差）常数不同
  - 这个常数动一动就是偏差-方差分解
  - 当然动到$E\bar X$则有某项“最小”。这可以帮助理解记忆“分解”的含义
- 推广
  - 变成高维，相应方差变成2范数等，乘积变成内积等……（略）
  - Bregman Loss
    - $\phi: \mathbb R^d \to \mathbb R$严格凸可导
    - 诱导Bregman Loss Function $l_\phi(\theta,\hat \theta)=\phi(\theta) - \phi(\hat\theta) - (\theta - \hat \theta)^T \nabla \phi(\hat\theta)$，相当于泰勒高阶项
    - 正定（来自严格凸），不一定对称
    - 当$\phi=\frac 12 ||\theta||_2^2$，退化成普通的MSE
      - 这个很好理解，因为二次函数肯定可以精确地二阶泰勒展开
    - 限制$\theta$向量各维正（比如有实际物理意义），则$\phi=\sum \theta_ilog\theta_i$时，代入化简……再代入$\theta$各维和为1，得到[[kl-divergence]]