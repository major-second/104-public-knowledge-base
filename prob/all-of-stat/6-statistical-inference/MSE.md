- 前置[[2-estimation]]
  - MSE：$E_\theta (\hat \theta - \theta)^2$，也就是估计出的参数和实际参数之间的平方差的期望
  - MSE是评估估计量好坏的一种方法，它的值越小，说明估计量越准确
- 参考[[优良标准]]
# bias-variance tradeoff
- 前置
  - [[tradeoff]]
  - [[variance#与$EX^2$关系]]
  - [[moment]]
- 参考[[转动惯量#平行轴定理]]
- 特殊情况
  - $\hat \mu = \bar X$
    - 参数$\mu$的估计量$\hat \mu$是统计量$\bar X$
  - $E(\bar X-\mu)^2=E((\bar X-E\bar X)+(E\bar X-\mu))^2=Var\bar X+0(交叉项)+Bias^2$
  - 注意$E\bar X-\mu$是常数，记为了偏差bias
- 一般情况：$\hat \theta$
  - $E(\hat \theta - \theta)^2 = E((\hat\theta - E(\hat \theta))^2 +(E(\hat\theta)-\theta)^2)(交叉项为0)=Var\hat \theta+Bias^2$
- 实质
  - 方差(Var)即中心[[moment]]
  - 均方误差(Mean Squared Error, MSE)
  - 原点[[moment]]
  - 实际上就是相对谁作差的那个常数不同，分别是$E\hat \theta,\theta(真实值),0$
  - 这个常数动一动就是偏差-方差分解
  - 当然动到$E\hat \theta$则有某项“最小”，是某种意义“正交投影”
  - 这可以帮助理解记忆“分解”的含义
- 这个方差开方，也称为[[standard-error]]
- 偏差和方差都趋向于0
  - 推出MSE趋向于0
  - 根据[[converge-in-l2]]
  - 至少有弱[[相合性]]
## 推广
- 变成[[high-dimension]]，相应方差变成2范数等，乘积变成内积等……（略）
- Bregman Loss
  - $\phi: \mathbb R^d \to \mathbb R$严格凸可导
  - 诱导Bregman Loss Function $l_\phi(\theta,\hat \theta)=\phi(\theta) - \phi(\hat\theta) - (\theta - \hat \theta)^T \nabla \phi(\hat\theta)$，相当于泰勒高阶项
  - 正定（来自严格凸），不一定对称
  - 当$\phi=\frac 12 ||\theta||_2^2$，退化成普通的MSE
    - 这个很好理解，因为二次函数肯定可以精确地二阶泰勒展开
  - 限制$\theta$向量各维正（比如有实际物理意义），则$\phi=\sum \theta_ilog\theta_i$时，代入化简……再代入$\theta$各维和为1，得到[[kl-divergence]]