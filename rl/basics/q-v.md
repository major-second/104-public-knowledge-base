前置[[mdp]]
[参考](https://zhuanlan.zhihu.com/p/109498587)
Q, V共同点：一个节点出发，下面所有节点的收获的期望值
值|V|Q|
-|-|-|
考察“节点”|状态|状态+动作|
助记|VS|QA|
是什么期望|子节点（动作）Q期望|子节点（状态）V期望
公式|$V_\pi(s)=\sum_a\pi(a\|s) Q_\pi (s,a)$|$Q_\pi(s,a)=R(s,a)+\gamma\sum _{s'} \Sigma p(s,a,s')V_\pi(s')$
备注说明|和策略相关。因为策略决定接下来动作分布|动作本身产生的奖励需要计算。概率是环境决定的不是策略决定的。但$Q$也和策略相关

原始方法计算：直接[[dp]]倒推。计算量过大，且需要已知环境等