# 子集搜索与评价
- 相关特征、无关特征、特征选择、特征工程、数据预处理
  - 维数灾难（属性过多）
  - 和降维目的类似
- 无关和“冗余”：冗余是有关的，但能被别人推出来，有时对应中间概念，有助于学习！
- 核心：子集的生成和搜索
- 生成
  - 最简单的：逐个增加（前向）或逐个减少（后向）
  - 两边同时：双向
- 评价
  - $V$个类，信息增益[[information-gain]]，$Ent(D)-\sum_v \frac{D^v}{D}Ent(D^v)$
    - 即：熵减信息熵
  - 实际中$V$如果很大，往往要基于前一步生成的特征子集进行计算
  - 更一般地：特征子集带来的划分和实际划分对比即可
- 生成：前向，决策：信息熵，那就很像[[4-decision-tree]]了
# 过滤式选择
- 先特征选择，再学
- Relief
  - 对每个示例$x_i$，找离它最近的对的，和离它最近的错的
  - 这个“近”用什么距离$diff$？如连续的规范化到$[0,1]$，离散的就是相等0不等1
  - 最后看$\sum_i (diff(x_i^j,x_{i,nm}^j)^2 - diff(x_i^j, x_{i,nh}^j)^2)$
    - 使用全部或采样出一些样本（用$i$表示）
    - 用于考察$j$特征
    - 看这个统计量，找“相关性”大的那些即可
    - 开销显然和采样次数、特征数呈线性关系
  - 扩展到多分类：near-miss对所有$k-1$种“错误可能”都做，计算统计量时加权，即
    - $\sum_i(\sum_{l\ne k} p_l diff(x_i^j, x_{i,l,nm}^j)^2-diff(x_i^j,x_{i,nh}^j)^2)$
# 包裹式选择
- 不代理指标，而是直接把最终性能作为指标
- 性能好，开销大
- LVW:
  - 拉斯维加斯算法（不断试，有解就合题意，但如果时间不够久就给不出解）
    - 注：蒙特卡洛是时间不够一定能给出解，但不一定合题意
  - 其实相当naive，就是不断碰，碰到比原来的好就更新当前最佳，所以是拉斯维加斯
  - 什么叫好？字典式，先看性能再看数量
# 嵌入式选择与$L_1$正则化
- 训练过程中同时选择
- 如岭回归，最小化的除了平方和，还有一项$\lambda ||w||_2^2$
- 如LASSO (Least Absolute Shrinkage and Selection Operator)
- 两者都利于降低过拟合，但$L_1$画出图比较“方”，容易取到角点（稀疏解）
  - 当然$L_0$最理想，但是不连续不好用
  - 这种“稀疏”自然就给出了选择结果
- 近端梯度下降
  - $min f(x)+\lambda ||x||_1$，$f$可导，$\nabla f$李普希茨连续
  - $x_k$附近把$f$泰勒展开（且设海瑟矩阵近似为$L/2$）
  - 则$\hat f(x)\approx f(x_k)+\langle \nabla f(x_k),x-x_k\rangle + \frac L2 ||x-x_k||^2$
  - 配方，发现最小值在$x_{k+1}=x_k-\frac 1L\nabla f(x_k)$取得
  - 这和梯度下降在形式上对齐
  - 所以可以类比：对$L_1$正规化后的问题，每次迭代可以写成$x_{k+1}=argmin_x \frac L2 ||x-z||_2^2+\lambda ||x||_1$，多加了一项
  - 这可以分类讨论解析解了。具体地，$|z^i|\le \lambda/L$时就出现0，特征被滤除
  - 注：关键是发现$||x-z||^2$没有“交叉项”，这是[[forall]]思想