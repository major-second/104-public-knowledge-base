# 子集搜索与评价
- 相关特征、无关特征、特征选择、特征工程、数据预处理
  - 维数灾难（属性过多）
  - 和降维目的类似
- 无关和“冗余”：冗余是有关的，但能被别人推出来，有时对应中间概念，有助于学习！
- 核心：子集的生成和搜索
- 生成
  - 最简单的：逐个增加（前向）或逐个减少（后向）
  - 两边同时：双向
- 评价
  - $V$个类，信息增益[[information-gain]]，$Ent(D)-\sum_v \frac{D^v}{D}Ent(D^v)$
    - 即：熵减信息熵
  - 实际中$V$如果很大，往往要基于前一步生成的特征子集进行计算
  - 更一般地：特征子集带来的划分和实际划分对比即可
- 生成：前向，决策：信息熵，那就很像[[4-decision-tree]]了
# 过滤式选择
- 先特征选择，再学
- Relief
  - 对每个示例$x_i$，找离它最近的对的，和离它最近的错的
  - 这个“近”用什么距离$diff$？如连续的规范化到$[0,1]$，离散的就是相等0不等1
  - 最后看$\sum_i (diff(x_i^j,x_{i,nm}^j)^2 - diff(x_i^j, x_{i,nh}^j)^2)$
    - 使用全部或采样出一些样本（用$i$表示）
    - 用于考察$j$特征
    - 看这个统计量，找“相关性”大的那些即可
    - 开销显然和采样次数、特征数呈线性关系
  - 扩展到多分类：near-miss对所有$k-1$种“错误可能”都做，计算统计量时加权，即
    - $\sum_i(\sum_{l\ne k} p_l diff(x_i^j, x_{i,l,nm}^j)^2-diff(x_i^j,x_{i,nh}^j)^2)$
# 包裹式选择
- 不代理指标，而是直接把最终性能作为指标
- 性能好，开销大
- LVW:
  - 拉斯维加斯算法（不断试，有解就合题意，但如果时间不够久就给不出解）
    - 注：蒙特卡洛是时间不够一定能给出解，但不一定合题意
  - 其实相当naive，就是不断碰，碰到比原来的好就更新当前最佳，所以是拉斯维加斯
    - 突然想到：[[4-probability]]中fair probability from an unfair coin也是拉斯维加斯
  - 什么叫好？字典式，先看性能再看数量
# 嵌入式选择与$L_1$正则化
- 训练过程中同时选择
- 如岭回归，最小化的除了平方和，还有一项$\lambda ||w||_2^2$
- 如LASSO (Least Absolute Shrinkage and Selection Operator)
- 两者都利于降低过拟合，但$L_1$画出图比较“方”，容易取到角点（稀疏解）
  - 当然$L_0$最理想，但是不连续不好用
  - 这种“稀疏”自然就给出了选择结果
- 近端梯度下降
  - $min f(x)+\lambda ||x||_1$，$f$可导，$\nabla f$李普希茨连续
  - $x_k$附近把$f$泰勒展开（且设海瑟矩阵近似为$L/2$）
  - 则$\hat f(x)\approx f(x_k)+\langle \nabla f(x_k),x-x_k\rangle + \frac L2 ||x-x_k||^2$
  - 配方，发现最小值在$x_{k+1}=x_k-\frac 1L\nabla f(x_k)$取得
  - 这和梯度下降在形式上对齐
  - 所以可以类比：对$L_1$正规化后的问题，每次迭代可以写成$x_{k+1}=argmin_x \frac L2 ||x-z||_2^2+\lambda ||x||_1$，多加了一项
  - 这可以分类讨论解析解了。具体地，$|z^i|\le \lambda/L$时就出现0，特征被滤除
  - 注：关键是发现$||x-z||^2$没有“交叉项”，这是[[forall]]思想
# 稀疏表示与字典学习
- 另一种稀疏性：数据矩阵中很多零元素
  - 如：绝大多数样本中都没有某个生僻字，绝大多数时刻都没有交易量，等
- 适当的稀疏性可能能引起线性可分，例如用支持向量机处理文本
- 字典学习（稀疏编码）：学到“字典”，对样本稀疏表达（升维）
  - 这当然和[[5-neural-network]]有很强联系
  - 注意区分于[[第十章]](todo)的降维
- $min_{B,\alpha_i} \sum_i ||x_i-B\alpha_i||_2^2+\lambda \sum_i ||\alpha_i||_1$
  - $\alpha$维数高（稀疏表示），被$B$映射后维数低了
  - $\alpha$维数$k$可调
- 学习方法：迭代（交替）优化，每次固定一个优化另一个（这很常见吧）
  - 固定$B$，那就LASSO做法，使得$\alpha_i$稀疏
  - 固定$\alpha_i$，那么就变成优化矩阵$min_B ||X-BA||^2_F$，F是Frobenius范数
  - KSVD：$X-BA = X-\sum(B的列\cdot A的行)$
  - 每次只更新第$i$列$b_i$，则可再变形，写作$常数矩阵 - b_i\alpha ^i$
  - 本来此时让一个矩阵减去一个秩为1矩阵，想让范数最小，显然应该[[svd]]一下（参考[[svd]]的讲解）
  - 但是如果直接取svd结果，显然可能破坏$\alpha$稀疏性
  - 所以tricky一下，$\alpha$只看非零元素，常数矩阵$E_i$只看$b_i\alpha^i$非零元素乘积对应项
  - 对比这里的字典学习和LASSO：算法有类似的地方（近端梯度下降），但一个是无监督地学表示，一个监督地让$w^Tx$接近$y$