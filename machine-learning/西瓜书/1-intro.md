- 学习的形式化定义：利用经验E提升在任务类T上关于评价标准P的性能
# 引言、基本术语
- “模型”：学习到的结果（例如一棵决策树）
  - 有些资料：模型是全局，模式是局部
- 常见术语：dataset, instance, sample, attribute, feature, attribute value, attribute space (sample space), feature vector
  - 在样本空间中采样。上下文判断“样本”指的是一个数据点还是整个数据集
- 约定记号：$\{x_1,\cdots,x_m\}$为$m$个样本，其中$x_i = (x_{i1};\cdots;x_{ij})$，分号表示列向量
- 训练，训练集，假设空间，假设，真相（真实），在假设空间中根据（指定了参数的）算法使模型接近真实。算法、参数、数据集都会影响训练结果
- 示例+标记：“样例” $(x_i,y_i)$，标记空间，输出空间
- 分类，回归，二分类，正类，负类。（看输出空间是啥）
- 聚类，簇（可能对应潜在的语义，但我们一般聚类前不知道。聚类的样本通常不具有标记信息），无监督
- 测试，测试样本，测试标记
- 泛化能力：适用于整个样本空间
  - 无论是典型的监督（分类回归）还是典型的无监督（聚类），都希望在测试样本上也生效
  - 假设样本空间中全体样本服从未知$\mathcal D$，独立同分布
# 假设空间
- 归纳学习：广义从样例学习，狭义学得概念（难，研究的少）
- 比如学习好/坏瓜二分类概念
- 死记硬背，机械学习
- 空间中搜索出匹配(fit)的假设
  - 比如把假设写成树，$色=*,根=*,声=*$的一个子节点是$色=青,根=*,声=*$等
  - 搜索策略：上至下/下至上/同时等
- 多个假设都符合：版本空间version space
# 归纳偏好
- “偏好”
  - 在可行的版本空间中选择模型，如喜欢“特殊”，“一般”，看重什么特征等
    - 更广义的：人类专家知识得到的先验、超参、损失项等。比如数据包含噪声，假设空间中没有能fit的假设，就需要设计归纳偏好了
    - 例如：认为相似的样本有相似的输出
  - 这里的“看重特征”是归纳偏好的一部分，和纯从样本中进行[[11-feature-selection]]不同
- 奥卡姆剃刀，平滑，简单
- 不同问题当然需要不同偏好
  - 总误差和学习算法无关。没有免费午餐！参考[[1-intro-NFL]]
# 发展历程
- 逻辑理论（逻辑推理） -> 知识工程、专家系统（总结出来教，困难） ->机器学习
- 连接主义、符号主义
- 1980，IWML（后来ICML），知名期刊ML，AI
- 历史上的分类
  - 机械学习：存储检索
  - 示教、类比：书中没咋说清楚
  - 样例中学习：本书主要关注
- 几种典型的样例中学习
  - 符号主义：决策树、逻辑、ILP，用逻辑表达式归纳。80年代是一大主流
    - ILP特点：表达能力太强，复杂度高，容易利用领域知识，容易精化增强。问题规模大就不行，90年代后寒冬了
  - 神经网络：50年代兴起，Simon断言人工智能就是符号化建模。“不能处理异或”。后来复兴。特点：黑箱调参玄学
  - 统计学习：90年代中期兴起，SVM，核方法（不过六七十年代就有了）
- 统计学习打符号，深度学习打传统机器学习，都和算力、存储能力有关！
# 应用现状、阅读材料
- 数据挖掘：统计学界。一开始带贬义。现在：可以理解为数据库 + 数据分析（机器学习、统计学）
- 自然科学探索色彩：联系“人类如何学习”
- 迁移学习恰似类比学习在统计学习大发展后的升级版
- 深度学习思想上并未显著超越80年代研究
- 概念学习、泛化特化搜索、决策树、blockworld
- 奥卡姆剃刀 vs. 多释原则（保留和经验观察一致的所有假设）principle of multiple explanations，参考[[8-ensemble]]
- 会议：ICML, NIPS, COLT
- 期刊：ML, JMLR
- 人工智能领域：IJCAI, AAAI, AI, JAIR
- 数据挖掘领域：KDD, ICDM
- 计算机视觉CVPR
- 统计Annals of Statistics
- Samuel跳棋程序：强化学习，不显式编程，非数值运算，对IBM指令集产生影响