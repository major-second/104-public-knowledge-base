# 基本形式、线性回归
- $f(x) = w^T x +b$
- 参考[[4-regression]]
- 好处
  - 非线性模型可由线性模型加层级结构、高维映射得
  - 有可解释性
- 使用[[2-eval]]说的均方误差，那就最小二乘法，在[[unary]], [[multi-ary]]中都有详细推导
- 属性是离散的
  - 有序可以适当表达成实数
  - 否则转化为$k$维向量
  - 如第一类为$(1,0,0)$，以此类推
  - one-hot
- [[multi-ary]]不满秩情况：如生物信息学中基因
  - 这样就有很多组$w$能让均方误差最小化（联想[[西瓜书/1-intro]]的版本空间）
  - 比如可以引入正则化项，例如参考[[11-feature-selection]]
- 广义线性模型：$y = g^{-1}(w^T x +b)$，其中$g$单调连续且充分光滑
  - 例如$g=ln$，则实际操作中就是$x$和$lny$做回归
  - 注：对$x$做变换则是“特征工程”了
    - 联想：刚刚说的“属性离散”和接下来说的“分类问题”的区别
  - 这时经常用加权最小二乘法或[[mle]]而不是原始的最小二乘法
# 对数几率回归
- 广义线性模型的应用：分类问题，逻辑（对数几率）回归
  - 对数几率函数$y = \frac 1{1+e^{-x}}$
    - 其光滑，且近似于$0\to 0.5\to 1$，“正1负0”的函数
    - 有对称性
    - 像S（sigmoid）
  - 模型：$y = \frac 1 {1+e^{-(w^T x +b)}}$
    - 实际回归时：$g(y)=ln\frac y{1-y} = w^T x+b$
    - $g(y)$是“对数几率”（logit, logistic），取值$\mathbb R$
    - $y/(1-y)$是“几率”，取值$\mathbb R^+$
  - “逻辑回归”这个名字可能有误导性，跟逻辑毫无关系
  - 说是回归，其实是分类
- 好处
  - 无需假设数据分布
  - 有近似概率输出
  - 对率函数性质好，可使用许多优化算法
- 求解：回忆前面所说，这里往往就不用最小二乘法了，而是[[mle]]
  - 写出对数似然函数：只需回忆$p(y=1|x)=e^a/(1+e^a), p(y=0|x)=1/(1+e^a),其中a=w^T x+b$，就可以容易写出并化简，得到关于参数$\beta:=(w;b)$的高阶可导连续凸函数
  - 从而可用[[4-local-search]]，[[optimization]]等
  - 如梯度下降、牛顿法等
# 线性判别分析